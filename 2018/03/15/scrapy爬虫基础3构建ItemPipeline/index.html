<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
        scrapy爬虫基础3：构建ItemPipeline - Smilettech&#39;s Blog
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/aircloud.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>
</head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i>  </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar">
            <img src="/img/1.png" />
        </div>
        <div class="name">
            <i>Yu</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont icon-shouye1"></i>
                    <span>主页</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/archives">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>关于</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#4、构建处理数据的流水线Pipeline"><span class="toc-text">4、构建处理数据的流水线Pipeline</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#实现Item-Pipeline"><span class="toc-text">实现Item Pipeline</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#启用Item-Pipeline"><span class="toc-text">启用Item Pipeline</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#更多例子"><span class="toc-text">更多例子</span></a></li></ol></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>
        <div class="index-about-mobile">
            <i>  </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        scrapy爬虫基础3：构建ItemPipeline
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2018-03-15 21:15:06</span></span>

        
        <span class="attr">标签：/
        
        <a class="tag" href="/tags/#爬虫" title="爬虫">爬虫</a>
        
        
        </span>
    </div>

    <div class="post-content no-indent">
        <h2 id="4、构建处理数据的流水线Pipeline"><a href="#4、构建处理数据的流水线Pipeline" class="headerlink" title="4、构建处理数据的流水线Pipeline"></a>4、构建处理数据的流水线Pipeline</h2><p>　　在scrapy中，Item Pipeline是处理数据的组件,也是一个类，一个Item Pipeline只负责一种功能的数据处理(典型功能是①清洗数据 ②验证数据有效性 ③过滤重复数据 ④将数据存入数据库)。在一个项目中可以同时启用多个Item Pipeline，它们按指定次序级联起来，形成一条数据处理流水线。</p>
<h3 id="实现Item-Pipeline"><a href="#实现Item-Pipeline" class="headerlink" title="实现Item Pipeline"></a>实现Item Pipeline</h3><p>运行爬虫，查看爬取的信息(前五行)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$Scrapy crawl books -o books.csv</span><br><span class="line">$head -5 books.csv    #查看文件开头的5行</span><br></pre></td></tr></table></figure></p>
<p>　　如果想把书价改为人民币价格(英镑价格×汇率)，在pipelines.py中实现PriceConverterPipeline<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class PriceConverterPipeline(objedct):    # 不需要继承特定基类,只需要实现某些特定方法</span><br><span class="line">    exchange_rate = 8.5309</span><br><span class="line"></span><br><span class="line"># 一个Item Pipeline必须实现一个process_item方法,它用来处理每项由Spider爬取到的数据</span><br><span class="line"># 2个参数：①Item：爬取到的一项数据（Item或字典） ②Spider：爬取此项数据的Spider对象</span><br><span class="line">    def process_item(self,item,spider):</span><br><span class="line">        # ①提取item的price字段  ②去掉前面英镑符号,转换为float类型,乘以汇率</span><br><span class="line">        price = float(item[&apos;price&apos;][1:])*self.exchange_rate</span><br><span class="line">        item[&apos;price&apos;]=&apos;￥%.2f&apos;%price   #保留2位小数，赋值回item的price字段</span><br><span class="line">        return item     #返回被处理过的item</span><br></pre></td></tr></table></figure></p>
<p>　　【补充1】关于process_item方法返回的item：①返回的数据会递送给下一级Item Pipeline继续处理；②如果process_item在处理某项item时抛出一个DropItem异常(一般在检测到无效数据/想过滤的数据时会抛出它)，该项Item便会被抛弃，不再递送给后面的Item Pipeline继续处理，也不会导出到文件。<br>　　【补充2】除了必须实现的process_item方法外，还有3个比较常用的方法(可根据需求选择实现)：①open_spider(self,spider)→Spider打开时（处理数据前）回调该方法，通常该方法用于在开始处理数据之前完成某些初始化工作，如连接数据库；②close_spider(self,spider)→Spider关闭时（处理数据后）回调该方法，通常该方法用于在处理完所有数据之后完成某些清理工作，如关闭数据库；③from_crawler(cls,crawler)→创建Item Pipeline对象时回调该类方法。一般在该方法中通过crawler.settings读取配置，根据配置创建Item Pipeline对象。</p>
<h3 id="启用Item-Pipeline"><a href="#启用Item-Pipeline" class="headerlink" title="启用Item Pipeline"></a>启用Item Pipeline</h3><p>　　因为Item Pipeline是可选组件，所以如果想启用某个/些Item Pipeline，需要在settings.py中进行设置。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES=&#123;&apos;bookspider.pipelines.PriceConverterPipeline&apos;:300,&#125;</span><br></pre></td></tr></table></figure></p>
<p>　　ITEM_PIPELINES是一个字典，将想启用的Item Pipeline添加到这个字典中，key是每个Item Pipeline类的导入路径，value是一个数值(0~1000),当同时启用多个Item Pipeline时,scrapy会根据这些数值决定每个Item Pipeline处理数据的先后顺序(数值小的先启用)。启用PriceConverterPipeline后，重新运行爬虫，并观察结果</p>
<h3 id="更多例子"><a href="#更多例子" class="headerlink" title="更多例子"></a>更多例子</h3><p>　　【例1】过滤重复数据<br>　　为了确保爬取到的书籍信息中没有重复项，可以实现一个去重Item Pipeline。这里，我们就以书名作为主键进行去重，实现DuplicatesPipeline<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from scrapy.exceptions import DropItem</span><br><span class="line">class DuplicatesPipeline(object):</span><br><span class="line">    def __init__(self):     #增加构造器方法,初始化用于对书名去重的集合</span><br><span class="line">        self.book_set=set()</span><br><span class="line">    def process_item(self,item,spider):</span><br><span class="line">        name=item[&apos;name&apos;]      #先取出item的name字段</span><br><span class="line">        if name in self.book_set:      #检查书名是否已在集合book_set中</span><br><span class="line">            raise DropItem(&quot;Duplicate book found:%s&quot;%item) #如果存在,就是重复数据,抛出DropItem异常,将item抛弃</span><br><span class="line">        self.book_set.add(name)     #否则,将item的name字段存入集合</span><br><span class="line">        return item</span><br></pre></td></tr></table></figure></p>
<p>　　接下来测试DuplicatesPipeline。先在不启用它的情况下运行爬虫，查看结果（此时有1000本书）；然后在设置文件中启用它，运行爬虫发现有999本，说明有两本书是同名的，翻越爬虫的log信息可以找到重复值。<br>　　【例2】将数据存入MongoDB<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">form scrapy.item import Item</span><br><span class="line">imort pymongo</span><br><span class="line">class MongoDBPipeline(object):</span><br><span class="line">     DB_URI = &apos;mongodb://localhost:27017/&apos;     #在类属性中定义2个变量,这个是数据库URI地址</span><br><span class="line">     DB_NAME = &apos;scrapy_data&apos;     #数据库名称</span><br><span class="line"></span><br><span class="line">#在Spider整个爬取过程中，数据库的连接和关闭操作只需要进行一次(处理数据之前连接,处理完后关闭)</span><br><span class="line">#下面2个方法,在Spider打开和关闭时被调用</span><br><span class="line">     def open_spider(self,spider):</span><br><span class="line">          self.client = pymongo.MongoClient(self.DB_URI)</span><br><span class="line">          self.db = self.client[self.DB_NAME]</span><br><span class="line"></span><br><span class="line">     def close_spider(self,spider):</span><br><span class="line">          self.client.close()</span><br><span class="line"></span><br><span class="line">#在process_item中实现MongoDB数据库的写入操作</span><br><span class="line">     def process_item(self,item,spider):</span><br><span class="line">          collection = self.db[spider.name]    #得到一个集合</span><br><span class="line">          post = dict(item) if isinstance(item,Item) else item</span><br><span class="line">          collection.insert_one(post)    #将数据插入该集合,集合对象的insert_one方法需传入一个字典对象,不能传入Item对象，所以在调用前先对item的类型进行判断:如果item是Item对象就转换为字典</span><br><span class="line">          return item</span><br></pre></td></tr></table></figure></p>
<p>　　接下来测试MongoDBPipeline，在设置文件中启用它，运行爬虫并查看数据库中的结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$scrapy crawl books</span><br><span class="line">$mongo</span><br></pre></td></tr></table></figure></p>
<p>　　如果想通过设置文件得到数据库的URI地址、名字，需要在MongoDBPipeline中添加下面代码，并删除在类属性中定义的DB_URI和DB_NAME<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">@classmethod</span><br><span class="line">def from_crawler(cls,crawler): #cls是MongoDBPipeline类对象，crawler的settings属性可访问设置文件</span><br><span class="line">    cls.DB_URI = crawler.settings.get(&apos;MONGO_DB_URI&apos;,&apos;mongodb://localhost:27917/&apos;)</span><br><span class="line">    cls.DB_NAME = crawler.settings.get(&apos;MONGO_DB_NAME&apos;,&apos;scrapy_data&apos;) #将读取的信息赋给cls属性</span><br><span class="line">    return cls()</span><br></pre></td></tr></table></figure></p>
<p>　　然后在设置文件setings.py中，对所要使用的数据库进行设置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MONGO_DB_URI = &apos;mongodb://192.168.1.105:27017/&apos;</span><br><span class="line">MONGO_DB_NAME = &apos;liushuo_scrapy_data&apos;</span><br></pre></td></tr></table></figure></p>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>
    </div>
</div>
    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        

        

    </ul>
    
    <p>
        Created By <a href="https://hexo.io/">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer>




</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = ""
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


</html>

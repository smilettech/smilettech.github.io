<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
        scrapy爬虫进阶1：模拟登陆 - Smilettech&#39;s Blog
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/aircloud.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>
</head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i>  </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar">
            <img src="/img/1.png" />
        </div>
        <div class="name">
            <i>Yu</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont icon-shouye1"></i>
                    <span>主页</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/archives">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>关于</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#了解网站登录的原理"><span class="toc-text">了解网站登录的原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#用FormRequest模拟登录"><span class="toc-text">用FormRequest模拟登录</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#（1）构造FormRequest对象，即构造含有表单数据的请求"><span class="toc-text">（1）构造FormRequest对象，即构造含有表单数据的请求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（2）提交表单请求"><span class="toc-text">（2）提交表单请求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（3）设计Spider代码：爬取用户信息-实现登录"><span class="toc-text">（3）设计Spider代码：爬取用户信息,实现登录</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#识别验证码"><span class="toc-text">识别验证码</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#（1）用pytesseract识别验证码（OCR识别技术）"><span class="toc-text">（1）用pytesseract识别验证码（OCR识别技术）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（2）验证码识别平台"><span class="toc-text">（2）验证码识别平台</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（3）人工识别"><span class="toc-text">（3）人工识别</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cookie登录"><span class="toc-text">Cookie登录</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#（1）获取浏览器Cookie"><span class="toc-text">（1）获取浏览器Cookie</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（2）CookiesMiddleware源码分析"><span class="toc-text">（2）CookiesMiddleware源码分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（3）实现BrowserCookiesMiddleware"><span class="toc-text">（3）实现BrowserCookiesMiddleware</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#例子：爬取知乎个人信息"><span class="toc-text">例子：爬取知乎个人信息</span></a></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>
        <div class="index-about-mobile">
            <i>  </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        scrapy爬虫进阶1：模拟登陆
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2018-03-19 20:36:15</span></span>

        
        <span class="attr">标签：/
        
        <a class="tag" href="/tags/#爬虫" title="爬虫">爬虫</a>
        
        
        </span>
    </div>

    <div class="post-content no-indent">
        <p>　　有些网站需要用户登录之后才能爬取数据，此时scrapy爬虫需要先模拟用户登录。</p>
<h2 id="了解网站登录的原理"><a href="#了解网站登录的原理" class="headerlink" title="了解网站登录的原理"></a>了解网站登录的原理</h2><p>　　【请求】浏览器会向服务器发送<strong>含有登录表单数据</strong>的HTTP请求。具体来说：”表单”对应于HTML中的form节点，当用户填写完表单提交后，浏览器会根据form节点的内容发送请求。<strong>表单数据</strong>应包含的信息：账号 + 密码信息 + 3个隐藏信息(由多个键值对构成，每个键值对对应一个input元素，键→name,值→value)<br>　　　form节点的属性：①<strong>method</strong> 决定HTTP请求方法(一般是POST)<br>　　　　　　　　　　　②<strong>action</strong> 决定http请求的url<br>　　　　　　　　　　　③<strong>enctype</strong> 决定表单数据的编码类型<br>　　　form中的input节点属性：①<strong>email</strong>(账号) ②<strong>password</strong>(密码)<br>　　　　隐藏的input属性：在div style=”display:none;”中，比如①name=”_next”(登录成功后页面跳转的地址) ②name=”_formkey”(防止CSRF跨域攻击)<br>　　【响应】响应头部中Set-Cookie字段,就是服务器保存在浏览器的Cookie信息，其中包含标识用户身份的session信息，之后对该网站发送的其他HTTP请求都会带上这个“身份证”，服务器程序通过这个“身份证”识别出发送请求的用户，从而决定响应怎样的页面。</p>
<h2 id="用FormRequest模拟登录"><a href="#用FormRequest模拟登录" class="headerlink" title="用FormRequest模拟登录"></a>用FormRequest模拟登录</h2><h3 id="（1）构造FormRequest对象，即构造含有表单数据的请求"><a href="#（1）构造FormRequest对象，即构造含有表单数据的请求" class="headerlink" title="（1）构造FormRequest对象，即构造含有表单数据的请求"></a>（1）构造FormRequest对象，即构造含有表单数据的请求</h3><p>　　【方法1】直接构造FormRequest对象：先把表单信息收集到一个字典中，然后使用<strong>表单数据字典</strong>构造FormRequest对象。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;scrapy shell 登录url</span><br><span class="line">&gt;&gt;sel = response.xpath(&apos;//div[@style]/input&apos;)    #&lt;input&gt;中的信息(包含隐藏的)</span><br><span class="line">&gt;&gt;fd = dict(zip(sel.xpath(&apos;./@name&apos;).extract(),sel.xpath(&apos;./@value&apos;).extract())</span><br><span class="line">&gt;&gt;fd[&apos;email&apos;]=&apos;邮箱&apos;</span><br><span class="line">&gt;&gt;fd[&apos;password&apos;]=&apos;密码&apos;</span><br><span class="line">&gt;&gt;from scrapy.http import FormRequest</span><br><span class="line">&gt;&gt;request=FormRequest(&apos;登录url&apos;,formdata=fd)  #用formdata接受表单数据字典</span><br></pre></td></tr></table></figure></p>
<p>　　【方法2】FormRequest的<strong>from_response</strong>方法：参数是<strong>Response对象</strong>、账号和密码的表达数据字典，该方法能解析Rresponse对象的form节点,将隐藏在input中的信息自动填入表单数据。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;fd=&#123;&apos;email&apos;:&apos;邮箱&apos;,&apos;password&apos;:&apos;密码&apos;&#125;</span><br><span class="line">&gt;&gt;request=FormRequest.from_response(response,formdata=fd)</span><br></pre></td></tr></table></figure></p>
<h3 id="（2）提交表单请求"><a href="#（2）提交表单请求" class="headerlink" title="（2）提交表单请求"></a>（2）提交表单请求</h3><p>　　运行fetch(request)，得到log信息，可以看到POST请求的响应状态码为303，之后scrapy自动再发送一个GET请求(携带了第一个POST请求获取的Cookie信息)下载跳转页面，可以在浏览器中查看页面或者查看是否有特殊字符串,来验证登录是否成功。</p>
<h3 id="（3）设计Spider代码：爬取用户信息-实现登录"><a href="#（3）设计Spider代码：爬取用户信息-实现登录" class="headerlink" title="（3）设计Spider代码：爬取用户信息,实现登录"></a>（3）设计Spider代码：爬取用户信息,实现登录</h3><p>　　把模拟登录和爬取内容的代码分离开，使得逻辑上更加清晰<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from scrapy.http import Request,FormRequest</span><br><span class="line">class LoginSpider(scrapy.Spider):</span><br><span class="line">    name = &apos;login&quot;</span><br><span class="line">    allowed_domains=[&apos;example.webscraping.com&apos;]</span><br><span class="line">    start_urls=[&apos;http:/....&apos;]</span><br><span class="line">    def parse(self,response):</span><br><span class="line">       #解析登录后下载的页面，此例中为用户个人信息页面</span><br><span class="line">        keys = response.css(&apos;table label::text&apos;).re(&apos;(.+):&apos;)</span><br><span class="line">        values-response.css(&apos;table td.w2p_fw::text&apos;).extract()</span><br><span class="line">        yield dict(zip(keys,values))</span><br><span class="line">    login_url=&apos;登录页面的url&apos;</span><br><span class="line">   #覆写基类的start_requests方法，最先请求登录页面</span><br><span class="line">    def start_requests(self):</span><br><span class="line">        yield Request(self.login_url,callback=self.login)</span><br><span class="line">    def login(self,response):     #登录页面的解析函数，在该方法中进行模拟登录，构造表单请求并提交</span><br><span class="line">        fd = &#123;&apos;email&apos;:&apos;邮箱&apos;,&apos;password&apos;:&apos;密码&apos;&#125;</span><br><span class="line">        yield FormRequest.from_response(response,formdata=fd,callback=self.parse_login)</span><br><span class="line">    def parse_login(self,response):      #登录成功后，处理响应</span><br><span class="line">        if &apos;Welcome Liu&apos; in response.text:     #页面是否有他是字符串</span><br><span class="line">            yield from super().start_requests()  #如果成功则调用基类的start_requests方法,继续爬取start_urls中的页面</span><br></pre></td></tr></table></figure></p>
<h2 id="识别验证码"><a href="#识别验证码" class="headerlink" title="识别验证码"></a>识别验证码</h2><p>　　很多网站为了防止爬虫爬取，登录时需要用户输入验证码。①验证码图片对应一个img ②与账号密码输入框一样,验证码输入框也对应一个input节点，所以用户输入的验证码会成为表单数据的一部分，表单提交后由服务器验证。识别验证码有很多方式，常用的有以下3种</p>
<h3 id="（1）用pytesseract识别验证码（OCR识别技术）"><a href="#（1）用pytesseract识别验证码（OCR识别技术）" class="headerlink" title="（1）用pytesseract识别验证码（OCR识别技术）"></a>（1）用pytesseract识别验证码（OCR识别技术）</h3><p>　　OCR是光学字符识别，用于在图像中提取文本信息。tesseract-ocr是用OCR技术实现的一个验证码识别库，在Python中可以通过第三方库pytesseract调用它(先安装tesseract-ocr和pytesseract)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#用pytesseract识别图片code.png中的验证码</span><br><span class="line">from PIL import Image</span><br><span class="line">import pytesseract</span><br><span class="line">img=Image.open(&apos;code.png&apos;)</span><br><span class="line">img=img.convert(&apos;L&apos;)    #为提高识别率,调用Image对象的convert方法把图片转换为黑白图</span><br><span class="line">pytesseract.image_to_string(img)     #将黑白图传递给image_to_string方法进行识别</span><br></pre></td></tr></table></figure></p>
<p>　　以之前的LoginSpider为模板，实现一个使用pytesseract识别验证码登录的Spider<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from scrapy import Request,FormRequest</span><br><span class="line">import json</span><br><span class="line">from PIL import Image</span><br><span class="line">from io import BytesIO</span><br><span class="line">import pytesseract</span><br><span class="line">from scrapy.log import logger</span><br><span class="line"></span><br><span class="line">class CaptchaLoginSpidr(scrapy.Spider):</span><br><span class="line">    name = &quot;login_captcha&quot;</span><br><span class="line">    start_urls = [&apos;http://XXX.com/&apos;]</span><br><span class="line">    def parse(self,response):</span><br><span class="line">        ……</span><br><span class="line">        login_url=&apos;登录页面url&apos;</span><br><span class="line">        user = &apos;邮箱&apos;</span><br><span class="line">        password = 密码</span><br><span class="line">        def start_requests(self):</span><br><span class="line">            yield Request(self.login_url,callback=self.login,dont_filter=True)</span><br><span class="line"></span><br><span class="line">#带有验证码的登录，需额外发送一个请求→获取验证码图片。login方法处理登录页面的响应、下载验证码图片的响应</span><br><span class="line">#当response.meta[&apos;login_response&apos;]存在时，response是验证码图片的响应；否则是登录页面的响应</span><br><span class="line"></span><br><span class="line">    def login(self,response):</span><br><span class="line">        login_response = response.meta.get(&apos;login_response&apos;)</span><br><span class="line">        if not login_response:     # 解析登录页面时</span><br><span class="line">            captchaUrl = response.css(&apos;label.field.prepend-icon img::attr(src)&apos;).extract_first()</span><br><span class="line">            captchaUrl = response.urljoin(captchaUrl)       # 提取验证码图片的url</span><br><span class="line">          # 发送请求下载图片,并将登录页面的Response对象保存到Request对象的meat字典中</span><br><span class="line">            yield Request(captchaUrl,callback=self.login,meta=&#123;&apos;login_response&apos;:response&#125;,dont_filter=True)</span><br><span class="line"></span><br><span class="line">        else:     #处理下载验证码图片的响应</span><br><span class="line">           # 识别图片验证码的方法get_captcha_by_OCR(图片二进制数据)</span><br><span class="line">            formdata = &#123; &apos;email&apos;:self.user,&apos;pass&apos;:self.password, &apos;code&apos;:self.get_captcha_by_OCR(response.body),&#125;</span><br><span class="line">           # 将之前保存的登录页面的Response对象取出，构造FormRequest对象并提交</span><br><span class="line">            yield FormRequest.from_response(login_response, callback=self.parse_login, formdata=formdata, dont_filter=True)</span><br><span class="line"></span><br><span class="line">    def parse_login(self,response):     # 处理表单请求的响应,响应正文是一个json串,包含了用户验证结果</span><br><span class="line">        info = json.loads(response.text)     # 用json.loads将正文转换为Python字典</span><br><span class="line">        if info[&apos;error&apos;]==&apos;0&apos;:        # 根据error字段的值,判断登录是否成功</span><br><span class="line">            logger.info(&apos;登录成功:-)&apos;)</span><br><span class="line">            return super().start_requests()     # 登录成功,就从start_urls中的页面开始爬取</span><br><span class="line">        logger.info(&apos;登录失败:-(,重新登录...&apos;)</span><br><span class="line">        return self.start_requests()</span><br><span class="line"></span><br><span class="line">    def get_captcha_by_OCR(self,data):     # OCR识别,data是图片二进制数据(bytes)</span><br><span class="line">        img = Image.open(BytesIO(data))       # 先把二进制转换成类文件对象,再用Image.open函数构造Image对象</span><br><span class="line">        img = img.convert(&apos;L&apos;)        # 转换成黑白图</span><br><span class="line">        captcha = pytesseract.image_to_string(img)      # 识别</span><br><span class="line">        img.close()</span><br><span class="line">        return captcha</span><br></pre></td></tr></table></figure></p>
<h3 id="（2）验证码识别平台"><a href="#（2）验证码识别平台" class="headerlink" title="（2）验证码识别平台"></a>（2）验证码识别平台</h3><p>　　对于某些复杂的验证码，方法(1)的识别率很低或者无法识别。目前有很多网站(验证码识别平台)专门提供验证码识别服务，可以识别较为复杂的验证码（有些是人工处理的），这些平台多数是付费使用的，价格大约为1元钱识别100个验证码。平台提供了HTTP服务接口，用户可以通过HTTP请求将验证码图片发送给平台，平台识别后将结果通过HTTP响应返回。</p>
<h3 id="（3）人工识别"><a href="#（3）人工识别" class="headerlink" title="（3）人工识别"></a>（3）人工识别</h3><p>　　在其他识别方式不管用时，人工识别一次验证码也是可行的。将人工识别的方式也加入CaptchaLoginSpider中，再添加一个get_captcha_by_user方法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def get_captcha_by_user(self,data):</span><br><span class="line">    img = Image.open(BytesIO(data))    #scrapy下载完验证码图片后,得到Image对象</span><br><span class="line">    img.show()      #将图片显示出来</span><br><span class="line">    captcha=input(&apos;输入验证码:&apos;)      #用Python内置的input函数，等待用户肉眼识别后输入识别结果</span><br><span class="line">    img.close()</span><br><span class="line">    return captcha</span><br></pre></td></tr></table></figure></p>
<h2 id="Cookie登录"><a href="#Cookie登录" class="headerlink" title="Cookie登录"></a>Cookie登录</h2><p>　　目前网站的验证码越来越复杂，某些验证码已经复杂到人类难以识别的程度，有时提交表单登录的路子难以走通。此时我们可以换一种登录爬取的思路，在使用浏览器登录网站后，包含用户身份信息的Cookie会被浏览器保存在本地，如果scrapy爬虫能直接使用浏览器中的Cookie发生HTTP请求，就可以绕过提交表单登录的步骤。</p>
<h3 id="（1）获取浏览器Cookie"><a href="#（1）获取浏览器Cookie" class="headerlink" title="（1）获取浏览器Cookie"></a>（1）获取浏览器Cookie</h3><p>　　不用研究种浏览器将Cookie以哪种形式存储在哪里，可以用第三方Python库browsercookie，就能获取chrome和firefox浏览器中的Cookie<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import browsercookie</span><br><span class="line">chrome_cookiejar=browsercookie.chrome()      #获取chrome浏览器中的cookie</span><br><span class="line">firefox_cookiejar=browsercookie.firefox()      #获取firefox浏览器中的cookie</span><br><span class="line">type(chrome_cookiejar)</span><br><span class="line">for cookie in chrome_cookiejar:</span><br><span class="line">    print(cookie)</span><br></pre></td></tr></table></figure></p>
<p>　　browsercookie的chrome和firefox方法分别返回chrome和firefox浏览器中的cookie，返回值是一个http.cookiejar.CookieJar对象，对CookieJar对象进行迭代，可以访问其中的每个Cookie对象</p>
<h3 id="（2）CookiesMiddleware源码分析"><a href="#（2）CookiesMiddleware源码分析" class="headerlink" title="（2）CookiesMiddleware源码分析"></a>（2）CookiesMiddleware源码分析</h3><p>　　scrapy爬虫所使用的cookie由内置下载中间件CookiesMiddleware自动处理。下面我们来分析一下CookiesMiddleware是如何工作的，源码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">import os,six,logging</span><br><span class="line">from collections import defaultdict</span><br><span class="line">from scrapy.exceptions import NotConfigured</span><br><span class="line">from scrapy.http import Response</span><br><span class="line">from scrapy.http.cookies import CookieJar</span><br><span class="line">from scrapy.utils.python import to_native_str</span><br><span class="line"></span><br><span class="line">logger=logging.getLogger(__naem__)</span><br><span class="line">class CookiesMiddleware(object):</span><br><span class="line">    def __init__(self,debug=False):</span><br><span class="line">        self.jars=defaultdict(CookieJar)     #创建一个默认字典，每一项都是一个scrapy.http.cookies.CookieJar对象</span><br><span class="line">        self.debug=debug</span><br><span class="line"></span><br><span class="line">#CookiesMiddleware能让爬虫同时使用多个不同CookieJar，比如在某网站我们注册了两个账号</span><br><span class="line">#若想同时登陆两个账号爬取网站，为了避免Cookie冲突，可以让每个账号发生的HTTP请求使用不同的CookieJar</span><br><span class="line">#在构造Requset对象时，参数meta=&#123;&apos;cookiejar&apos;:&apos;账号&apos;&#125;，表示该账号发送的请求</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls,crawler):</span><br><span class="line">        if not crawler.settings.getbool(&apos;COOKIES_ENABLED&apos;);   #从设置文件读取COOKIES_ENABLED,决定是否启用中间件</span><br><span class="line">            raise NotConfigured      #如果不启用，则抛出NotConfigured异常，Scrapy将忽略该中间件</span><br><span class="line">        return cls(crawler.settings.getbool(&apos;COOKIES_DEBUG&apos;))      #如果启用，调用构造器创建对象</span><br><span class="line"></span><br><span class="line">    def process_request(self,request,spider):      #处理每一个待发送的Request对象</span><br><span class="line">        if request.meta.get(&apos;dont_merge_cookies&apos;,False):</span><br><span class="line">            return</span><br><span class="line">        cookiejarkey=request.meta.get(&quot;cookiejar&quot;)     #获取用户指定的CookieJar，未指定则用CookieJar(self.jars[None])</span><br><span class="line">        jar=self.jars[cookiejarkey]</span><br><span class="line">        cookies=self._get_request_cookies(jar,request)     #获取发送请求request应携带的Cookie信息，填写到HTTP请求</span><br><span class="line">        for cookie in cookies:</span><br><span class="line">            jar.set_cookie_if_ok(cookie,request)</span><br><span class="line">        request.headers.pop(&apos;Cookie&apos;,None)</span><br><span class="line">        jar.add_cookie_header(request)</span><br><span class="line">        self._debug_cookie(request,spider)</span><br><span class="line"></span><br><span class="line">    def process_response(self,request,response,spider):    #处理每一个Response对象</span><br><span class="line">        if request.meta.get(&apos;dont_merge_cookies&apos;,False):</span><br><span class="line">            return response</span><br><span class="line">        cookiejarkey=request.meta.get(&apos;cookiejar&apos;)    #获取CookieJar对象</span><br><span class="line">        jar=self.jars[cookiejarkey]</span><br><span class="line">        jar.extract_cookies(resposne,request)     #将HTTP响应头部中的Cookie信息保存到CookieJar对象中</span><br><span class="line">        self._debug_set_set_cookie(response,spider)</span><br><span class="line">        return response</span><br><span class="line">#注意：这里的CookieJar是scrapy.http.cookies.CookieJar</span><br><span class="line">#而第一节的CookieJar是标准库中的http.cookiejar.CookieJar，是不同的类</span><br><span class="line">#前者对后者进行了包装，两者可以互相转化</span><br><span class="line"></span><br><span class="line">    def _debug_cookie(self,request,spider):</span><br><span class="line">        if self.debug:</span><br><span class="line">            cl=[to_native_str(c,errors=&apos;replace&apos;)</span><br><span class="line">                for c in request.headers.getlist(&apos;Cookie&apos;)]</span><br><span class="line">            if cl:</span><br><span class="line">                cookies=&quot;\n&quot;.join(&quot;Cookie:&#123; &#125;\n&quot;.format(c) for c in cl)</span><br><span class="line">                msg=&quot;Sending cookies to:&#123; &#125;\n&#123; &#125;&quot;.format(request,cookies)</span><br><span class="line">                logger.debug(msg,extra=&#123;&apos;spider&apos;:spider&#125;)</span><br><span class="line"></span><br><span class="line">    def _debug_set_cookie(self,response,spider):</span><br><span class="line">        if self.debug:</span><br><span class="line">            cl=[to_native_str(c,errors=&apos;replace&apos;)</span><br><span class="line">                for c in response.headers.getlist(&apos;Set-Cookie&apos;)]</span><br><span class="line">            if cl:</span><br><span class="line">                cookies=&quot;\n&quot;.join(&quot;Set-Cookie:&#123; &#125;\n&quot;.format(c) for c in cl)</span><br><span class="line">                msg=&quot;Received cookies from:&#123; &#125;\n&#123; &#125;&quot;.format(response,cookies)</span><br><span class="line">                logger.debug(msg,extra=&#123;&apos;spider&apos;:spider&#125;)</span><br><span class="line"></span><br><span class="line">    def _format_cookie(self,cookie):</span><br><span class="line">        cookie_str=&apos;%s=%s&apos;%(cookie[&apos;name&apos;],cookie[&apos;value&apos;])</span><br><span class="line">        if cookie.get(&apos;path&apos;,None):</span><br><span class="line">            cookie_str += &apos;;Path=%s&apos;%cookie[&apos;path&apos;]</span><br><span class="line">        if cookie.get(&apos;domain&apos;,None):</span><br><span class="line">            cookie_str += &apos;;Domain=%s&apos;%cookie[&apos;domain&apos;]</span><br><span class="line">        return cookie_str</span><br><span class="line"></span><br><span class="line">    def _get_request_cookies(self,jar,request):</span><br><span class="line">        if isinstance(request.cookies,dict):</span><br><span class="line">            cookie_list=[&#123;&apos;name&apos;:k,&apos;value&apos;:v&#125; for k,v in six.iteritems(request.cookies)]</span><br><span class="line">        else:</span><br><span class="line">            cookie_list=request.cookies</span><br><span class="line">        cookies=[self._format_cookie(x) for x in cookie_list]</span><br><span class="line">        headers = &#123;&apos;Set-Cookie&apos;:cookies&#125;</span><br><span class="line">        response=Response(request.url,headers=headers)</span><br><span class="line">        return jar.make_cookies(response,request)</span><br></pre></td></tr></table></figure></p>
<h3 id="（3）实现BrowserCookiesMiddleware"><a href="#（3）实现BrowserCookiesMiddleware" class="headerlink" title="（3）实现BrowserCookiesMiddleware"></a>（3）实现BrowserCookiesMiddleware</h3><p>CookiesMiddleware自动处理头部Cookie的特性给用户提供了便利，但它不能使用浏览器的Cookie，我们可以利用browsercookie对CookiesMiddleware进行改良，实现一个能使用浏览器Cookie的中间件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import browsercookiess</span><br><span class="line">from scrapy.downloadermiddlewares.cookies import CookiesMiddleware</span><br><span class="line"></span><br><span class="line">class BrowserCookiesMiddleware(CookiesMiddleware):     #继承CookiesMiddleware并实现构造器方法</span><br><span class="line">    def __init__(self,debug=Fales):</span><br><span class="line">        super().__init__(debug)</span><br><span class="line">        self.load_browser_cookies()     #加载浏览器Cookie</span><br><span class="line"></span><br><span class="line">#浏览器中的Cookie存储在CookieJar字典中self.jars中</span><br><span class="line">    def load_browser_cookies(self):</span><br><span class="line">        jar = self.jars[&apos;chrome&apos;]        #从默认字典中获得CookieJar对象</span><br><span class="line">        chrome_cookiejar = browsercookie.chrome()   #获得chrome浏览器中的cookie,填入CookieJar对象中</span><br><span class="line">        for cookie in chrome_cookiejar:</span><br><span class="line">            jar.set_cookie(cookie)</span><br><span class="line">        jar = self.jars[&apos;firefox&apos;]</span><br><span class="line">        forefox_cookiejar = browsercookie.firefox()</span><br><span class="line">        for cookie in firefox_cookiejar:</span><br><span class="line">            jar.set_cookie(cookie)</span><br></pre></td></tr></table></figure></p>
<h2 id="例子：爬取知乎个人信息"><a href="#例子：爬取知乎个人信息" class="headerlink" title="例子：爬取知乎个人信息"></a>例子：爬取知乎个人信息</h2><p>　　通过例子展示BrowserCookiesMiddleware的使用：在Chrome浏览器登录知乎后，可以访问用户个人信息页面，然后我们用BrowserCookiesMiddleware爬取这个登录后才能访问的页面，提取用户的“姓名”和“个性域名” 信息<br>　　　①创建项目<br>　　　②将BrowserCookiesMiddleware源码赋值到该项目下的middlewares.py中，并在配置文件settings.py中添加如下配置：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#伪装成常规浏览器</span><br><span class="line">USER_AGENT=&apos;Mozilla/5.0(X11;Linux x86_64)Chrome/42.0.23311.90 Safari/537.36&apos;</span><br><span class="line">#用BrowserCookiesMiddleware替代CookiesMiddleware启用前者，关闭后者</span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">     &apos;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&apos;:None,</span><br><span class="line">     &apos;browser_cookie.middlewares.BrowserCookiesMiddleware&apos;:701,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>　　　③由于需求非常简单，所以不再编写Spider，直接在scrapy shell环境中进行演示。注意，为了使用项目中的配置，需要在项目目录下启动scrapy shell命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;scrpay shell</span><br><span class="line">&gt;&gt;from scrapy import Request</span><br><span class="line">&gt;&gt;url = &apos;网站&apos;</span><br><span class="line">&gt;&gt;fetch(Request(url,meta=&#123;&apos;cookiejar&apos;:&apos;chrome&apos;&#125;))</span><br><span class="line">&gt;&gt;view(response)</span><br></pre></td></tr></table></figure></p>
<p>　　　调用了view函数后，在浏览器中看到页面。结果表明BrowserCookiesMiddleware按我们所预期的工作了，Scrapy爬虫使用浏览器的Cookie成功的获取了一个需要用户登录后才能访问的页面<br>　　　④提取页面中的“姓名”和“个性域名”信息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">resposne.css(&apos;div#rename-section span.name::text&apos;).extract_first()     #返回&quot;硕-奶酪&apos;</span><br><span class="line">response.xpath(&apos;string(//div[@id=&quot;js-url-preview&quot;])&apos;).extract_first()     #返回&apos;zhihu.com/people/shuo_cheese&apos;</span><br></pre></td></tr></table></figure></p>
<p>　　小结：学习了scrapy爬虫模拟登录网站的相关内容，首先介绍了网站登录的原理，并讲解如何使用FormRequest提交登录表单模拟登录，然后讲解识别验证码的3种方法，最后介绍如何使用浏览器Cookie直接登录，并实现了一个下载中间件BrowserCookiesMiddleware</p>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>
    </div>
</div>
    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        

        

    </ul>
    
    <p>
        Created By <a href="https://hexo.io/">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = ""
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


</html>

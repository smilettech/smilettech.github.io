<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
        scrapy爬虫基础4：下载文件和图片 - Smilettech&#39;s Blog
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/aircloud.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>
</head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i>  </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar">
            <img src="/img/1.png" />
        </div>
        <div class="name">
            <i>Yu</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont icon-shouye1"></i>
                    <span>主页</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/archives">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>关于</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#下载文件FilesPipeline"><span class="toc-text">下载文件FilesPipeline</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#下载图片ImagesPipeline"><span class="toc-text">下载图片ImagesPipeline</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#项目1：爬取matplotlib例子的源码文件"><span class="toc-text">项目1：爬取matplotlib例子的源码文件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#项目2：下载360图片"><span class="toc-text">项目2：下载360图片</span></a></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>
        <div class="index-about-mobile">
            <i>  </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        scrapy爬虫基础4：下载文件和图片
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2018-03-16 15:26:06</span></span>

        
        <span class="attr">标签：/
        
        <a class="tag" href="/tags/#爬虫" title="爬虫">爬虫</a>
        
        
        </span>
    </div>

    <div class="post-content no-indent">
        <p>　　scrapy提供了两个Item Pipeline，专门用于下载文件(FilesPipeline)和图片(ImagesPipeline)，可以将这两个Item Pipeline看做特殊的下载器。只需要通过item的一个特殊字段,将要下载的文件/图片的url传递进去，它们就能自动下载文件/图片到本地，并将下载结果存入item的另一个特殊字段。</p>
<h2 id="下载文件FilesPipeline"><a href="#下载文件FilesPipeline" class="headerlink" title="下载文件FilesPipeline"></a>下载文件FilesPipeline</h2><p>　　比如下载多本pdf格式的小说。<strong>步骤：</strong>①在设置文件中启用FilesPipeline，一般将其置于其它Item Pipeline之前→ITEM_PIPELINES={‘scrapy.pipelines.files.FilesPipeline’:1} ②在设置文件中使用FILES_STORE指定文件下载目录 ③在spider中将所有下载文件的url收集到一个列表，赋给item的file_urls字段（FilesPipeline在处理每项item时,会读取item[‘file_urls’],对其中每一个url进行下载）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#spider示例代码</span><br><span class="line">class DownloadBookSpider(scrapy.Spider):</span><br><span class="line">    ……</span><br><span class="line">    def parse(response):</span><br><span class="line">        item = &#123; &#125;</span><br><span class="line">        item[&apos;file_urls]=[ ]      #下载列表</span><br><span class="line">        for url in response.xpath(&apos;//a/@href&apos;).extract():</span><br><span class="line">            download_url=response.urljoin(url)</span><br><span class="line">            item[&apos;file_urls&apos;].append(download_url)      #将url填入下载列表</span><br><span class="line">        yield item</span><br></pre></td></tr></table></figure></p>
<p>　　当FilesPipeline下载完item[‘file_urls’]中的所有文件后，会将各文件的下载结果收集到另一个列表，并赋给item的files字段（item[‘files’]）。下载结果信息包含以下内容：①<strong>Path</strong> 文件下载到本地的路径（相对于FILES_STORE的相对路径）②<strong>Checksum</strong> 文件的校验和 ③<strong>url</strong> 文件的url地址</p>
<h2 id="下载图片ImagesPipeline"><a href="#下载图片ImagesPipeline" class="headerlink" title="下载图片ImagesPipeline"></a>下载图片ImagesPipeline</h2><p>　　图片也是文件，所以下载图片本质上也是下载文件，ImagesPipeline是FilesPipeline的子类，使用上和FilesPipeline大同小异，只是在所使用的item字段和配置选项上略有<strong>差别</strong>：①导入路径不同 → scrapy.pipelines.files.FilesPipeline 和 scrapy.pipelines.images.ImagesPipeline；②Item字段不同：file_urls,files 和 image_urls,images；③指定下载目录的变量不同：FILES_STORE 和 IMAGES_STORE<br>　　而且ImagesPipeline在FilesPipeline的基础上,针对图片增加了一些特有的功能,比如 ①为图片生成<strong>缩略图</strong>：在设置文件中设置IMAGES_THUMBS就能开启该功能，它是一个字典,每一项的值是缩略图的尺寸，比如IMAGES_THUMBS = {‘small’:(50,50),’big’:(270,270),}，下载1张图片,本地会出现3张图片→1张原图+2张缩略图；②<strong>过滤掉尺寸过小的图片</strong>：在设置文件中设置IMAGES_MIN_WIDTH(指定图片最小的宽)、IMAGES_MIN_HEIGHT(最小的高)就能开启该功能，比如IMAGES_MIN_WIDTH=110  IMAGES_MIN_HEIGHT=110，如果下载一张105×200图片,这张图片就会被抛弃,因为宽度不符合标准。</p>
<h2 id="项目1：爬取matplotlib例子的源码文件"><a href="#项目1：爬取matplotlib例子的源码文件" class="headerlink" title="项目1：爬取matplotlib例子的源码文件"></a>项目1：爬取matplotlib例子的源码文件</h2><p>（1）创建项目<br>　　<strong>项目需求：</strong>matplotlib网站上有很多例子，并可以下载代码(点击source code)，现在我想把所有例子的源码文件都下载到本地，可以编写一个爬虫程序完成这个任务。<br>　　<strong>页面分析：</strong>①先看如何在[例子列表页面]中获得所有[例子页面]的链接，方法是在cmd中输入scrapy shell url下载页面，再用view(response)在浏览器中查看页面，<strong>发现</strong>所有[例子页面]的链接,都在属性class为”toctree-12”的li标签中，可以用LinkExtractor提取所有例子页面的链接；②调用fetch函数下载第一个例子页面，并调用view函数在浏览器中查看页面，fetch(‘url’)，再用view查看下载页面，发现在第1个例子页面中的下载地址,在class为”reference external”的a标签中的href属性，用selector提取即可。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#所有例子页面：</span><br><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line">le = LinkExtractor(restrict_css=&apos;li.toctree-l2&apos;)</span><br><span class="line">links = le.extract_links(response)</span><br><span class="line">[link.url for link in links]</span><br><span class="line"></span><br><span class="line">#每个例子页面的下载链接：</span><br><span class="line">href = response.css(&apos;a.reference.external::attr(href)&apos;).extract_first()</span><br><span class="line">response.urljoin(href)</span><br></pre></td></tr></table></figure></p>
<p>　　<strong>创建项目：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;scrapy startproject matplotlib_examples</span><br><span class="line">&gt;&gt;cd matplotlib_examples</span><br><span class="line">&gt;&gt;scrapy genspider examples matplotlib.org</span><br></pre></td></tr></table></figure></p>
<p>（2）定义保存数据的容器Item<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">class ExampleItem(scrapy.Item):</span><br><span class="line">    file_urls = scrapy.Field()</span><br><span class="line">    files = scrapy.Field()</span><br></pre></td></tr></table></figure></p>
<p>（3）编写提取数据的爬虫Spider<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line"></span><br><span class="line">class ExamplesSpider(scrapy.Spider):</span><br><span class="line">    name = &apos;examples&apos;</span><br><span class="line">    allowed_domains = [&apos;matplotlib.org&apos;]</span><br><span class="line">    start_urls = [&apos;http://...&apos;]</span><br><span class="line">    def parse(self,response):</span><br><span class="line">        le = LinkExtractor(restrict_css=&apos;li.toctree-l2&apos;)</span><br><span class="line">        links = le.extract_links(response)</span><br><span class="line">        for link in links:</span><br><span class="line">            yield scrapy.Request(link.url,callback=self.parse_example)</span><br><span class="line"></span><br><span class="line">#下面是例子页面的解析函数,目的是获得下载链接,将其放入一个列表赋给ExampliItem的file_urls字段</span><br><span class="line">    def parse_example(self,response):</span><br><span class="line">        href = response.css(&apos;a.reference.external::attr(href)&apos;).extract_first()</span><br><span class="line">        url = response.urljoin(href)</span><br><span class="line">        example = ExampleItem()</span><br><span class="line">        example[&apos;file_urls&apos;] = [url]</span><br><span class="line">        return example</span><br></pre></td></tr></table></figure></p>
<p>（4）构建处理数据的流水线Pipeline<br>　　在settings.py中启用FilesPipeline，并设定文件下载目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES=&#123;</span><br><span class="line">    &apos;scrapy.pipelines.files.FilesPipeline&apos;:1,</span><br><span class="line">&#125;</span><br><span class="line">FILES_STORE=&apos;examples_src&apos;</span><br></pre></td></tr></table></figure></p>
<p>（5）运行爬虫，并查看examples_src目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$scrapy crawl examples -o examples.json</span><br><span class="line">$tree examples_src</span><br></pre></td></tr></table></figure></p>
<p>　　507个源码文件被下载到了exmaples_src/full目录下，并且每个文件的名字都是一串长度相等的奇怪数字，这些数字是下载文件url的sha1散列值，这种命名方式可以防止重名的文件相互覆盖，但是文件名太不直观。如果你想把这些例子文件按照类别下载不同目录下，可以修改FilesPipeline为文件命名的规则。阅读FilesPipeline源码发现，原来是其中的file_path方法决定了文件的命名<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class FilesPipeline(MediaPipeline):</span><br><span class="line">    ……</span><br><span class="line">    def file_path(self,request,response=None,info=None):</span><br><span class="line">    ……</span><br><span class="line">        if not isinstance(request,Request):</span><br><span class="line">            _warn()</span><br><span class="line">            url=request</span><br><span class="line">        else:</span><br><span class="line">            url=request.url</span><br><span class="line">        if not hasattr(self.file_key,&apos;_base&apos;):</span><br><span class="line">            _warn()</span><br><span class="line">            return self.file_key(url)</span><br><span class="line">        media_guid=hashib.sha1(to_bytes(url)).hexdigest()</span><br><span class="line">        media_ext=os.path.splitext(url)[1]</span><br><span class="line">        return &apos;full/%s%s&apos;%(media_guid,media_ext)</span><br></pre></td></tr></table></figure></p>
<p>　　现在实现一个FilesPipeline子类，覆写file_path方法来实现所期望的文件命名规则，这些源码文件url的最后两部分是类别和文件名，在pipelines.py实现MyFilesPipeline<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from scrapy.pipelines import FilesPipeline</span><br><span class="line">from urllib.parse import urlparse</span><br><span class="line">from os.path import basename,dirname,join</span><br><span class="line">class MyFielsPipeline(FilesPipeline):</span><br><span class="line">    def file_path(self,request,response=None,info=None):</span><br><span class="line">        path = urlparse(request.url).path</span><br><span class="line">        return join(basename(dirname(path)),basename(path))</span><br></pre></td></tr></table></figure></p>
<p>　　修改配置文件，使用MyFielsPipeline替代FielsPipeline<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES=&#123;</span><br><span class="line">    &apos;matplotlib_examples.pipelines.MyFielsPipeline&apos;:1,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>　　删除之前下载的所有文件，重新运行爬虫后再查看examples_src目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$rm -r examples_src/full</span><br><span class="line">$rm examples.json</span><br><span class="line">$scrapy crawl examples -o examples.json</span><br><span class="line">$tree examples_src</span><br></pre></td></tr></table></figure></p>
<p>　　结果507个文件按类别被下载到26个目录下</p>
<h2 id="项目2：下载360图片"><a href="#项目2：下载360图片" class="headerlink" title="项目2：下载360图片"></a>项目2：下载360图片</h2><p>（1）创建项目<br>　　<strong>项目需求：</strong>下载360图片网站中艺术分类下的所有图片到本地<br>　　<strong>页面分析：</strong>向下滚动鼠标滚轮便会有更多图片加载出来，图片加载是由javascript脚本完成的。jQuery发送的请求，其响应结果是一个json串。复制jQuery发送请求的url，使用scrapy shell进行访问，查看响应结果的内容（json）→ res = json.loads(response.body.decode(‘utf8’))<br>　　　响应结果（json）中的list字段是一个图片信息列表，count字段是列表中图片信息的数量，每一项图片信息的qhimg_url字段图片下载地址。<br>　　　url的规律：①ch参数：分类标签 ②sn参数：从第几张图片开始加载，即结果列表中第一张图片在服务器端的序号。可以通过这个API每次获取固定数量的图片信息，从中提取每一张图片的下载地址，直到响应结果中的count字段为0（意味着没有更多图片了）<br>　　<strong>创建项目：</strong>取名为so_image，再使用scrapy genspider命令创建Spider<br>（2）定义保存数据的容器Item：略<br>（3）编写提取数据的爬虫Spider<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from scrapy import Request</span><br><span class="line">import json</span><br><span class="line">class ImagesSpider(scrapy.Spider):</span><br><span class="line">    BASE_URL=&apos;...&apos;</span><br><span class="line">    start_index=0</span><br><span class="line">    MAX_DOWNLOAD_NUM=1000     #限制最大下载数量，防止磁盘用量过大</span><br><span class="line">    name = &apos;images&apos;</span><br><span class="line">    start_urls = [BASE_URL%0]</span><br><span class="line">    def parse(self,response):</span><br><span class="line">        infos=json.loads(response.body.decode(&apos;utf-8&apos;))      #使用json模块解析响应结果</span><br><span class="line">       #提取所有图片下载url到一个列表，赋给item的&apos;image_urls&apos;字段</span><br><span class="line">        yield &#123;&apos;image_urls&apos;:[info[&apos;qhimg_url&apos;] for info in infos[&apos;list&apos;]]&#125;</span><br><span class="line">       #如count字段大于0，并且下载数量不足MAX_DOWNLOAD_NUM，继续获取下一页图片信息</span><br><span class="line">        self.start_index += infos[&apos;count&apos;]</span><br><span class="line">        if infos[&apos;count&apos;]&gt;0 and self.start_index &lt; self.MAX_DOWNLOAD_NUM:</span><br><span class="line">             yield Request(self.BASE_URL%self.start_index)</span><br></pre></td></tr></table></figure></p>
<p>（4）构建处理数据的流水线Pipeline<br>　　　在settings.py中启用ImagesPipeline，并设定图片下载目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES=&#123;</span><br><span class="line">    &apos;scrapy.pipelines.ImagesPipeline&apos;:1,</span><br><span class="line">&#125;</span><br><span class="line">IMAGES_STORE=&apos;download_images&apos;</span><br></pre></td></tr></table></figure></p>
<p>（5）运行爬虫$scrapy crawl images</p>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>
    </div>
</div>
    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        

        

    </ul>
    
    <p>
        Created By <a href="https://hexo.io/">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer>




</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = ""
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


</html>

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
        scrapy爬虫基础2：编写Spider - Smilettech&#39;s Blog
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/aircloud.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>
</head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i>  </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar">
            <img src="/img/1.png" />
        </div>
        <div class="name">
            <i>Yu</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont icon-shouye1"></i>
                    <span>主页</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/archives">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>关于</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#3、编写提取数据的爬虫Spider"><span class="toc-text">3、编写提取数据的爬虫Spider</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#爬虫逻辑"><span class="toc-text">爬虫逻辑</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（1）Spider基类提供的功能"><span class="toc-text">（1）Spider基类提供的功能</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（2）为Spider命名"><span class="toc-text">（2）为Spider命名</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（3）设定起始爬虫url"><span class="toc-text">（3）设定起始爬虫url</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（4）网页解析"><span class="toc-text">（4）网页解析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（5）XPath"><span class="toc-text">（5）XPath</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（6）CSS"><span class="toc-text">（6）CSS</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（7）提取新url"><span class="toc-text">（7）提取新url</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#（8）构造下载请求"><span class="toc-text">（8）构造下载请求</span></a></li></ol></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>
        <div class="index-about-mobile">
            <i>  </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        scrapy爬虫基础2：编写Spider
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2018-08-15 21:15:06</span></span>

        
        <span class="attr">标签：/
        
        <a class="tag" href="/tags/#爬虫" title="爬虫">爬虫</a>
        
        
        </span>
    </div>

    <div class="post-content no-indent">
        <h2 id="3、编写提取数据的爬虫Spider"><a href="#3、编写提取数据的爬虫Spider" class="headerlink" title="3、编写提取数据的爬虫Spider"></a>3、编写提取数据的爬虫Spider</h2><p>　　实现一个Spider子类就像完成一系列填空题，先介绍如何填空，再详细讨论每个细节。</p>
<h3 id="爬虫逻辑"><a href="#爬虫逻辑" class="headerlink" title="爬虫逻辑"></a>爬虫逻辑</h3><p>　　编写spider之前，先考虑以下问题：①给爬虫起一个名字 ②爬虫从哪个/哪些页面开始爬取？③对于一个已下载的页面，提取其中的哪些数据？④爬取完当前页面后，接下来爬取哪个/哪些页面？<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from bookspider.items import BookItem</span><br><span class="line">class BooksSpider(scrapy.Spider):     # （1）继承scrapy.Spider</span><br><span class="line">    name = &quot;books&quot;      #（2）为Spider取名,即每个爬虫的唯一标识</span><br><span class="line">    start_urls = [&apos;http://books.toscrape.com/&apos;]      #（3）设定爬虫起始点,可以多个</span><br><span class="line"></span><br><span class="line">    def parse(self,response):      #（4）实现页面解析函数,提取数据</span><br><span class="line">        for sel in response.css(&apos;article.product_pod&apos;):      #每本书的信息，是书本信息的大框架</span><br><span class="line">            book = BookItem()</span><br><span class="line">            book[&apos;name&apos;] = sel.xpath(&apos;./h3/a/@title&apos;).extract_first()      #提取书名信息,（5）xpath语法</span><br><span class="line">            book[&apos;price&apos;] = sel.css(&apos;p.price_color::text&apos;).extract_first()      #提取书价信息,（6）css语法</span><br><span class="line">            yield book</span><br><span class="line"></span><br><span class="line">      #（7）提取新的待爬取url</span><br><span class="line">        next_url = response.css(&apos;ul.pager li.next a::attr(href)&apos;).extract_first()   #得到相对路径</span><br><span class="line">        if next_url:</span><br><span class="line">            next_url = response.urljoin(next_url)      #得到绝对路径</span><br><span class="line">            yield scrapy.Request(next_url,callback=self.parse)    #（8）构造下载请求</span><br></pre></td></tr></table></figure></p>
<h3 id="（1）Spider基类提供的功能"><a href="#（1）Spider基类提供的功能" class="headerlink" title="（1）Spider基类提供的功能"></a>（1）Spider基类提供的功能</h3><p>　　供engine调用的接口，比如用来创建Spider实例的类方法from_crawler<br>　　供用户使用的工具函数，比如log方法可以将调试信息输出到日志<br>　　供用户访问的属性，比如通过settings属性可以访问配置文件中的配置</p>
<h3 id="（2）为Spider命名"><a href="#（2）为Spider命名" class="headerlink" title="（2）为Spider命名"></a>（2）为Spider命名</h3><p>　　一个Scrapy项目可以实现多个Spider，所以应唯一标识每个Spider<br>　　执行scrapy crawl时就用到这个标识，告诉Scrapy用哪个Spider爬虫</p>
<h3 id="（3）设定起始爬虫url"><a href="#（3）设定起始爬虫url" class="headerlink" title="（3）设定起始爬虫url"></a>（3）设定起始爬虫url</h3><p>　　start_urls通常被实现成一个列表，放入所有起始爬取点的url。设定start_urls之后，Spider基类会自动构造并提交对应的Request对象，以下是Spider基类的源码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class Spider(object_ref):</span><br><span class="line">    ……</span><br><span class="line">    def start_requests(self):    #构造并提交Request对象</span><br><span class="line">        for url in self.start_urls:    #遍历起始爬取点(通过实例访问类属性)</span><br><span class="line">            yield self.make_requests_from_url(url)</span><br><span class="line">    def make_requests_from_url(self,url):     #真正构造Request对象</span><br><span class="line">        return Request(url,dont_filter=True)</span><br><span class="line">    def parse(self,response)</span><br><span class="line">        raise NotImplementedError</span><br><span class="line">    ……</span><br></pre></td></tr></table></figure></p>
<p>　　【补充1】BooksSpider类没有实现start_requests方法，所以引擎会调用Spider基类的start_requests方法产生下载请求；我们也可以在BooksSpider类中自定义start_requests方法，覆盖基类的方法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def start_requests(self):</span><br><span class="line">    yield scrapy.Request(&apos;http://books.toscrape.com/&apos;,</span><br><span class="line">                       callback=self.parse_book,</span><br><span class="line">                       headers=&#123;&apos;User-Agent&apos;:&apos;...&apos;&#125;,</span><br><span class="line">                       dont_filter=True)</span><br><span class="line">def parse_book(response):</span><br><span class="line">    改用parse_book作为回调函数</span><br><span class="line">    ……</span><br></pre></td></tr></table></figure></p>
<p>　　【补充2】Spider基类构造Request对象时,没有向callback传递页面解析函数，它的默认解析函数是parse方法。所以BooksSpider类必须实现parse方法，否则会调用Spider基类的parse方法，抛出NotImplementedError异常。<br>　　【补充3】起始爬取点可能有多个，经过start_requests方法实现后，会得到一个可迭代对象（每个元素都是Request对象），每次由yield语句返回一个Request对象。</p>
<h3 id="（4）网页解析"><a href="#（4）网页解析" class="headerlink" title="（4）网页解析"></a>（4）网页解析</h3><p>　　网页解析就是从页面中提取数据。有两个python模块能处理这一问题 ①BeautifulSoup（API简洁易用，速度慢） ②lxml（由c语言编写的,速度快,API复杂）。而scrapy的Selector类综合了上述两者的优点(它基于lxml库构建,并简化了API接口)，它在scrpay.selector模块→from scrapy.selector import Selector<br>　　在Scrapy中，是使用Selector对象来提取页面中的数据，使用步骤如下：<br>　　【第一步】创建Selector对象(有两种方法)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#方法一：用Selector构造器</span><br><span class="line">Selector对象 = Selector(text=&apos;网页文本字符串&apos;)</span><br><span class="line"></span><br><span class="line">#方法二：用HtmlResponse(在scrapy.html模块中)构造器 + Selector构造器</span><br><span class="line">响应 = Response(url=&apos;网址&apos;,body=&apos;网页文本字符串&apos;,encoding=&apos;utf-8&apos;)</span><br><span class="line">Selector对象 = Selector(response=响应)</span><br><span class="line"></span><br><span class="line">#方法三：直接使用Response对象内置的Selector对象</span><br><span class="line">#访问Response对象的selector属性时，Response对象会自动创建Selector对象，并缓存以便下次使用</span><br><span class="line">response =  HtmlResponse(...)</span><br><span class="line">response.selector</span><br></pre></td></tr></table></figure></p>
<p>　　【第二步】用Selector对象的XPath或CSS选择器，选中数据<br>　　　为了方便使用，<strong>可以直接对Response对象调用xpath/css选择器(即可以跳过第一步构造selector对象)</strong>。比如response.xpath(‘…’)，会返回一个SelectorList对象(它是一个列表,元素是Selector对象,可以用for循环迭代访问每一个Selector对象)。SelectorList对象也可以用XPath/CSS选择器,它会作用于每一个Selector对象。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># scrapy源码的相关实现</span><br><span class="line">class TextResponse(Response):</span><br><span class="line">    ……</span><br><span class="line">    def xpath(self,query,**kwargs):</span><br><span class="line">        return self.selector.xpath(query,**kwargs)</span><br><span class="line"></span><br><span class="line">    def css(self,query):</span><br><span class="line">        return self.selector.css(query)</span><br></pre></td></tr></table></figure></p>
<p>　　【第三步】提取数据<br>　　　<strong>extract()</strong>   返回Unicode字符串的列表<br>　　　<strong>extract_first()</strong>   只能SelectorList对象使用，用于返回第一个Selector对象的Unicode字符串，相当于s[0].extract()或者s.extract()[0]<br>　　　<strong>re()</strong>   用正则表达式提取选中内容的某部分<br>　　　<strong>re_first()</strong>   只能SelectorList对象使用</p>
<h3 id="（5）XPath"><a href="#（5）XPath" class="headerlink" title="（5）XPath"></a>（5）XPath</h3><p>　　XPath是XML路径语言（XML Path Language），是一种用来确定xml文档中某部分位置的语言。xml文档（html属于xml）是有一些列节点构成的树。<strong>节点常见类型：</strong>①根节点：整个文档树的根；②元素节点：html、body、div、p、a；③属性节点：href；④文本节点：Hello word、Click here。<strong>节点之间的关系：</strong>①父子 ②兄弟 ③祖先、后裔<br>　　【基础语法】<br>　　　　<strong>/节点</strong>   从根开始搜索所有子节点<br>　　　　<strong>//节点</strong>   从根开始搜索所有后代节点<br>　　　　<strong>点’.’</strong>   代表当前节点,’.//节点’会从当前节点开始搜索<br>　　　　<strong>text()</strong>   选中文本<br>　　　　<strong>@属性名</strong>   选中属性，比如选择中所有属性是某值的节点→节点[@属性=’值’]<br>　　【其他函数】<br>　　　　<strong>string(参数)</strong>   返回参数的字符串值，比如s.xpath(‘string(/html/body/a/strong)’).extract()，相当于s.xpath(‘/html/body/a/strong/text()’)。区别是：如果两个文本在不同的标签下，用string可以直接将不同地方的字符串连接起来，用text()会返回这些字符串组成的列表(不会连接起来)<br>　　　　<strong>contains(str1,str2)</strong>   判断str1是否包含str2，返回布尔值</p>
<h3 id="（6）CSS"><a href="#（6）CSS" class="headerlink" title="（6）CSS"></a>（6）CSS</h3><p>　　CSS是层叠样式表，其选择器可用来确定HTML文档中某部分的位置。与XPath比较：语法比XPath简单，但功能不如XPath强大（python内部会将CSS翻译成xpath标的调用xpath方法）<br>　　【基本语法】<br>　　　　<strong>.值</strong>   选中所有class为该值的元素<br>　　　　<strong>#值</strong>   选中所有id为该值的元素<br>　　　　<strong>节点</strong>   选中所有该节点<br>　　　　<strong>节点1,节点2</strong>   同时选中两节点<br>　　　　<strong>节点1 节点2</strong>   选中所有节点2(后代节点)<br>　　　　<strong>节点1&gt;节点2</strong>   选中所有节点2(子节点)<br>　　　　<strong>[属性]</strong>   选中属性,有值→[属性=值],包含该值→[属性~=值]<br>　　　　<strong>节点::text</strong>   选中节点下的所有文本<br>　　　　<strong>节点::attr(属性)</strong>   选择节点下的属性文本</p>
<h3 id="（7）提取新url"><a href="#（7）提取新url" class="headerlink" title="（7）提取新url"></a>（7）提取新url</h3><p>　　提取url有两种方法：①Selector：因为链接也是页面中的数据，所以可以用与提取数据相同的方法进行提取，适用于提取少量的url或者提取规则比较简单的情形；②LinkExtractor类：是一个专门用于提取url的类，适用于提取大量url或规则比较复杂的情形<br>　　本例用的是第一种方法提取下一页链接，先用CSS选择器选中包含下一页链接的a元素并获取其href属性，然后调用response.urljoin方法计算绝对url，最后构造Request对象并提交。接下来介绍一下如何用LinkExtractor提取链接。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line">class BooksSpider(scrapy.Spider):</span><br><span class="line">    ……</span><br><span class="line">    def parse(self,response):</span><br><span class="line">        #提取链接，下一页的url在ul.page&gt;li.next&gt;a里面</span><br><span class="line">        le = LinkExtractor(restrict_css=&apos;ul.pager li.next&apos;)    #创建一个LinkExtractor对象,输入提取规则。如果不传递任何参数,就提取页面所有链接</span><br><span class="line">        links = le.extract_links(response)     #该方法根据提取规则,在response对象所包含的页面中提取链接，返回一个列表,元素是Link对象</span><br><span class="line">        if links:</span><br><span class="line">            next_url = links[0].url        #用links[0]获取Link对象,它的url属性就是链接的绝对地址</span><br><span class="line">            yield scrapy.Request(next_url,callback=self.parse)      #构造Request对象并提交</span><br></pre></td></tr></table></figure></p>
<p>　　LinkExtractor构造器的各个参数：①<strong>allow</strong>=正则表达式(或列表),提取与正则表达式匹配的链接；②<strong>deny</strong> 与allow相反,排除与正则表达式匹配的链接；③<strong>allow_domains</strong>=域名(或列表),提取指定域名的链接；④<strong>deny_domains</strong>；⑤<strong>restrict_xpaths</strong>=XPath表达式(或列表),提取XPath表达式选中区域下的链接；⑥<strong>restrict_css</strong>；⑦<strong>tags</strong>=标签(或列表),提取指定标签内的链接,默认是[‘a’,’area’]；⑧<strong>attrs</strong>=属性(或列表),提取指定属性内的链接,默认是[‘href’]；⑨<strong>process_value</strong>=回调函数，用该函数处理提取到的每个链接,一般返回一个字符串。</p>
<h3 id="（8）构造下载请求"><a href="#（8）构造下载请求" class="headerlink" title="（8）构造下载请求"></a>（8）构造下载请求</h3><p>【构造Resquest对象(下载请求)的参数】<br>　　<strong>url</strong>    必选，是请求页面下载的url地址<br>　　<strong>callback</strong>    回调→页面解析函数，页面下载完成后调用页面解析函数<br>　　<strong>method</strong>    http请求的方法，默认为’GET’<br>　　<strong>headers</strong>    http请求的头部字典(dict类型)<br>　　<strong>body</strong>    http请求的正文<br>　　<strong>cookies</strong>    Cookie信息字典(dict类型)<br>　　<strong>meta</strong>    Request的元数据字典(dict类型)，与其他模块互传信息<br>　　<strong>encoding</strong>    url和body的默认编码是’utf-8’<br>　　<strong>priority</strong>    指定优先级，默认为0(值小的优先下载)<br>　　<strong>dont_filte</strong>    默认为False→避免重复下载，当页面随时间变化则改为true<br>　　<strong>errback</strong>    请求出现异常/出现http错误时的回调函数<br>【补充】Response对象(响应)<br>　　Response只是一个基类，根据响应内容的不同有如下子类：TextResponse、HtmlResponse、XmlResponse。当一个页面下载完成时，下载器根据http响应头部中的Content-Type信息创建子类对象（爬取网页的内容是html文本，创建的是HtmlResponse对象）<br>　　HtmlResponse对象的参数：<br>　　　<strong>url</strong>    响应的url地址(str)<br>　　　<strong>status</strong>    响应的状态码(int)<br>　　　<strong>headers</strong>    响应的头部(dict)，可用get/getlist方法访问<br>　　　<strong>body</strong>    响应的正文(bytes)<br>　　　<strong>text</strong>    响应的正文(str)=response.body.decode(response.encoding)<br>　　　<strong>encoding</strong>    正文的编码，可能从headers/正文中解析出来<br>　　　<strong>request</strong>    产生该响应的Request对象<br>　　　<strong>meta</strong>    response.request.meta=处理响应的函数,response.meta取信息<br>　　　<strong>selector</strong>    Selector对象用于在Response中提取数据<br>　　　<strong>xpath/css</strong>    选择器,用来提取数据,是response.selector.xpath/css的快捷方式<br>　　　<strong>urljoin</strong>    当url是相对地址,response.urljoin(url)得到绝对地址</p>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>
    </div>
</div>
    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        

        

    </ul>
    
    <p>
        Created By <a href="https://hexo.io/">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = ""
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


</html>
